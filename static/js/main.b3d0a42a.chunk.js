(this["webpackJsonpweed-datasets"]=this["webpackJsonpweed-datasets"]||[]).push([[0],{34:function(e,a,t){e.exports=t(45)},44:function(e,a,t){},45:function(e,a,t){"use strict";t.r(a);var n=t(0),i=t.n(n),o=t(30),s=t.n(o),r=t(62),l=t(66),c=t(69),d=t(70),m=t(64),u=t(67),h=t(68),p=t(65),g=t(15),f=t(17),b=t(10),y=t(18),v=t(19),w=t(20),_=function(e){function a(){return Object(f.a)(this,a),Object(y.a)(this,Object(v.a)(a).apply(this,arguments))}return Object(w.a)(a,e),Object(b.a)(a,[{key:"render",value:function(){return i.a.createElement("nav",{className:"navbar navbar-inverse "},i.a.createElement("div",{className:"container-fluid"},i.a.createElement("div",{className:"navbar-header"},i.a.createElement("button",{type:"button",className:"navbar-toggle","data-toggle":"collapse","data-target":"#myNavbar"},i.a.createElement("span",{className:"icon-bar"}),i.a.createElement("span",{className:"icon-bar"}),i.a.createElement("span",{className:"icon-bar"})),i.a.createElement("div",{className:"navbar-brand",id:"name"},"Weed Dataset Collection")),i.a.createElement("div",{className:"collapse navbar-collapse",id:"myNavbar"},i.a.createElement("ul",{className:"nav navbar-nav navbar-right"},i.a.createElement("li",null,i.a.createElement(g.b,{to:"/"},i.a.createElement("span",{className:"glyphicon glyphicon-home"})," Home")),i.a.createElement("li",null,i.a.createElement(g.b,{to:"/contact"},i.a.createElement("span",{className:"glyphicon glyphicon-envelope"})," Contact"))))))}}]),a}(i.a.Component),A=[{id:1,source:"https://cloudstor.aarnet.edu.au/plus/s/hsP33Yb69uvEfXY",name:"QUT Redlands Weeds Dataset",description:'A diverse dataset of 4 different weed species collected in Queensland Australia. Data is split in to 4 parts: Session 1, Session 2, Merged Training, Merged Testing. The dataset accompanies the following paper: "David Hall, Feras Dayoub, Tristan Perez, Chris McCool. 2018. A rapidly deployable classification system using visual data for the application of precision weed management"',author:"",data_type:"RGB images",num_of_images:"830 images",num_of_plants:"830",num_of_species:"4 (cotton, feathertop, sowthistle, wild oats)",data_source:"Images were taken from the Agbot II robotic platform traveling over a controlled field using a downwards facing IDS UI-1240SE 1.3 MP global shutter camera. Data collected at a rate of 5 Hz with each image covering approximately 1 square meter area.",location:"Queensland Government Department of Agriculture and Fisheries' Redlands Research Station, Australia.",condition_of_use:"The dataset can be used freely for non-commercial applications.",manually_labelled:"yes",citation:["David Hall, Feras Dayoub, Tristan Perez, Chris McCool. 2017. A transplantable system for weed classification by agricultural robotics","David Hall, Feras Dayoub, Jason Kulk, Chris McCool. 2017. Towards unsupervised weed scouting for agricultural robotics","Hall, David R. 2018. A rapidly deployable approach for automated visual weed classification without prior species knowledge."]},{id:2,source:"https://github.com/cwfid/dataset",name:"A crop/Weed Field dataset",description:'The Crop/Weed Field Image Dataset (CWFID) comprises field images, vegetation segmentation masks and crop/weed plant type annotations, and accompanies the following publication: "Sebastian Haug, J\xf6rn Ostermann: A Crop/Weed Field Image Dataset for the Evaluation of Computer Vision Based Precision Agriculture Tasks, CVPPP 2014 Workshop, ECCV 2014".',author:"Haug, Sebastian and Ostermann",data_type:"RGB images",num_of_images:"60 images",num_of_plants:"n/a",num_of_species:"1 (carrot)",data_source:"autonomous field robot Bonirob",location:"an organic farm in Germany",condition_of_use:"only be used for non-commercial research ",manually_labelled:"yes",citation:["Dheeman Saha. 2019. Development of Enhanced Weed Detection System with Adaptive Thresholding , K-Means and Support Vector Machine","Subbian Umamaheswari, Rahul Arjun, D. Meganathan. 2018. Weed Detection in Farm Crops using Parallel Image Processing","Dheeman Saha, Austin Hanson, Sung Y. Shin. 2016. Development of Enhanced Weed Detection System with Adaptive Thresholding and Support Vector Machine","Mads Dyrmann. 2015. Fuzzy C-means based plant segmentation with distance dependent threshold","Mulham Fawakherji, Ali Youssef, Domenico Daniele Bloisi, Alberto Pretto, Daniele Nardi. 2019. Crop and Weeds Classification for Precision Agriculture Using Context-Independent Pixel-Wise Segmentation","Radhika Kamath, Mamatha Balachandra, Srikanth Prabhu. 2019. Raspberry Pi as Visual Sensor Nodes in Precision Agriculture: A Study","Anders Krogh Mortensen, S\xf8ren Skovsen, Henrik Karstoft, Ren\xe9 Gislum . 2019. The Oil Radish Growth Dataset for Semantic Segmentation and Yield Estimation","Yanlei Xu, Zongmei Gao, Lav R. Khot, Xiaotian Meng, Qin Zhang. 2018. A Real-Time Weed Mapping and Precision Herbicide Spraying System for Row Crops","Tatiana M. Pinho, Jo\xe3o Paulo Coelho, Josenalde Oliveira, Jos\xe9 Boaventura-Cunha. 2018. An Overview on Visual Sensing for Automatic Control on Smart Farming and Forest Management"]},{id:3,source:"https://github.com/AlexOlsen/DeepWeeds",name:"DeepWeeds",description:'The DeepWeeds dataset is a popular Australian weed dataset for the work, "2019. DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning". It consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora. In our work, the dataset was classified to an average accuracy of 95.7% with the ResNet50 deep convolutional neural network.',author:"Alex Olsen, Dmitry A. Konovalov, Bronson Philippa, Peter Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney, Brendan Calvert, Mostafa Rahimi Azghadi & Ronald D. White",data_type:"colour images",num_of_images:"17509 images",num_of_plants:"n/a",num_of_species:"8 (native to Australia: \xa0Chinee apple, Snake weed, Lantana, Prickly acacia, Siam weed, Parthenium, Rubber vine and Parkinsonia)",data_source:"ground vehicle with camera attached",location:"across Queensland (Black River, Charters Towers, Cluden, Douglas, Hervey Range, Kelso, McKinlay and Paluma)",condition_of_use:"publicly available (free to use)",manually_labelled:"yes",citation:["Olsen, A., Konovalov, D.A., Philippa, B.\xa0et al.\xa02018. DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning.","Corey Lammie, Alex Olsen, Tony Carrick, Mostafa Rahimi Azghadi. 2019. Low-Power and High-Speed Deep FPGA Inference Engines for Weed Classification at the Edge","\u0141 Chechli\u0144ski, B Siemi\u0105tkowska, M Majewski\xa0- Sensors. 2019. A System for Weeds and Crops Identification\u2014Reaching over 10 FPS on Raspberry Pi with the Usage of MobileNets, DenseNet and Custom Modifications","Alessandrodos Santos Ferreira, Daniel MatteFreitas, Gercina Gon\xe7alvesda Silva, Hemerson Pistori, Marcelo Theophilo Folhesc. 2019. Unsupervised deep learning and semi-automatic data labeling in weed discrimination","Stavros G. Vougioukas, Spyros Fountas. 2019. Smart automation in the agri-food chain : State of the art , prospects and impacts on workforce demands"]},{id:4,source:"https://www.kaggle.com/gavinarmstrong/open-sprayer-images/data",name:"A collection of broad leaved dock images for weed sprayer",description:"The data set includes pictures of broad leaved docks and picture of the land without broad leaved docks. The dataset is orginally created to develop an open sourced autonomous land drone that will propel itself across the fields spraying weeds it can see with its mounted cameras.",author:"Gavin  Armstrong",data_type:"colour images",num_of_images:"6697 images",num_of_plants:"n/a",num_of_species:"1 (broad leaved docks)",data_source:"Lane drone with mounted camera",location:"unknown",condition_of_use:"free to use (open sourced)",manually_labelled:"yes",citation:["Gavin Armstrong. 2017. The One Sprayer project"]},{id:5,source:"https://vision.eng.au.dk/plant-seedlings-dataset/",name:"Plant Seedlings Dataset",description:'The Plant Seedlings Dataset contains images of approximately 960 unique plants belonging to 12 species at several growth stages. The database have been recorded at Aarhus University Flakkebjerg Research station in a collaboration between University of Southern Denmark and Aarhus University, originalled published in the paper "A Public Image Database for Benchmark of Plant Seedling Classification Algorithms" in 2016.',author:"Giselsson, Thomas Mosgaard and Dyrmann, Mads and Jorgensen, Rasmus Nyholm and Jensen, Peter Kryger and Midtiby, Henrik Skov",data_type:"RGB images ",num_of_images:"9.7GB of images",num_of_plants:"960",num_of_species:"12 (maze, common wheat, sugar beet, scentless mayweed, common chickweed, shepherd\u2019s purse, cleavers, charlock, fat hen, small-flowered cranesbill, black-grass, loose silky-bent)",data_source:"Ground cameras",location:"Denmark",condition_of_use:"free to use",manually_labelled:"yes",citation:["Thomas Mosgaard Giselsson, Rasmus Nyholm J\xf8rgensen, Peter Kryger Jensen, Mads Dyrmann, Henrik Skov Midtiby. 2016. A Public Image Database for Benchmark of Plant Seedling Classification Algorithms","Carlos A. Mamani Diaz, Edgar E. Medina Castaneda, Carlos A. Mugruza Vassallo. 2019. Deep Learning for Plant Classification in Precision Agriculture.","Catherine R. Alimboyong, Alexander A. Hernandez. 2019. An Improved Deep Neural Network for Classification of Plant Seedling Images","Heba Ahmed Elnemr. 2019. Convolutional Neural Network Architecture for Plant Seedling Classification","Catherine R. Alimboyong, Alexander A. Hernandez, Ruji P. Medina. 2018. Classification of Plant Seedling Images Using Deep Learning","Daniel K. Nkemelu, Daniel Omeiza, Nancy Lubalo. 2018. Deep Convolutional Neural Network for Plant Seedlings Classification","Mads Dyrmann. 2017. Automatic Detection and Classification of Weed Seedlings under Natural Light Conditions"]},{id:6,source:"https://plantvision.unl.edu/dataset",name:"UNL Plant Phenotyping Dataset A: Oanicoid Phenomap-1",description:"This dataset consists of imaging data from 31 diverse maize inbreds as well as a number of representative genotypes from other panicoid grain crops. Plants were imaged once per day using the UNL Lemnatec Scanalyzer 3D. The dataset contains high resolution RGB photos taken from the top, front, and side (90 degree rotation) for 176 total plants on from 2 days after planting to 28 days after planting.",author:"The Plant Vision Initiative",data_type:"high resolution RGB photos",num_of_images:"95.9GB of images",num_of_plants:"176",num_of_species:"31 different maize inbreds",data_source:"UNL\xa0Lemnatec Scanalyzer 3D",location:"USA",condition_of_use:"free to use",manually_labelled:"yes",citation:["Sruti Das Choudhury, Saptarsi Goswami, Srinidhi Bashyam, Ashok Samal, Tala Awada. 2017. Automated Stem Angle Determination for Temporal Plant Phenotyping Analysis","Jin-Gang Yu, Zhikai Liang, James C. Schnable, Ashok Sama, Vincent Stoerger, Sruti Das Choudhury. 2016. Automated Vegetative Stage Phenotyping Analysis of Maize Plants using Visible Light Images","Ashok Samal, Tala Awada, Sruti Das Choudhury, Srinidhi Bashyam, Yumou Qiu. 2018. Holistic and component plant phenotyping using temporal image sequence","Sruti Das Choudhury, Ashok Samal, and Tala Awada. 2019. Leveraging Image Analysis for High-Throughput Plant Phenotyping"]},{id:7,source:"https://plantvision.unl.edu/dataset",name:"UNL Plant Phenotyping Dataset B: Component Plant Phenotyping Dataset (UNL-CPPD)",description:"This dataset consists of images of 13 maize plants for the two views: side view 0 degree and side view 90 degree. Plants were imaged once per day using the visible light camera of the UNL Lemnatec Scanalyzer 3D high throughput phenotyping facility. The dataset contains original images, groundtruth and annotated images to facilitate research advancements in the filed of componenet phenotyping analysis.",author:"The Plant Vision Initiative",data_type:"high resolution RGB photos",num_of_images:"7.2GB of images",num_of_plants:"13",num_of_species:"1 (maize plant)",data_source:"UNL\xa0Lemnatec Scanalyzer 3D",location:"USA",condition_of_use:"free to use",manually_labelled:"yes",citation:["Ashok Samal, Tala Awada, Sruti Das Choudhury, Srinidhi Bashyam, Yumou Qiu. 2018. Holistic and component plant phenotyping using temporal image sequence","Sruti Das Choudhury, Ashok Samal, and Tala Awada. 2019. Leveraging Image Analysis for High-Throughput Plant Phenotyping"]},{id:8,source:"https://plantvision.unl.edu/dataset",name:"UNL 3D Plant Phenotyping Dataset C: (UNL-3DPPD)",description:"This dataset consists of images of 20 maize plants & 20 sorghum plants for the 10 views: side views 0 degree, 36 degree, ..., and 244 degree. Plants were imaged once per day using the visible light camera of the UNL Lemnatec Scanalyzer 3D high throughput phenotyping facility. The dataset contains original images, groundtruth and annotated images to facilitate research advancements in the filed of componenet phenotyping analysis",author:"The Plant Vision Initiative",data_type:"high resolution RGB photos",num_of_images:"49.6GB of images",num_of_plants:"40",num_of_species:"2 (maize plants and sorghum plants",data_source:"UNL\xa0Lemnatec Scanalyzer 3D",location:"USA",condition_of_use:"free to use",manually_labelled:"yes",citation:["Sruti Das Choudhury, Ashok Samal, and Tala Awada. 2019. Leveraging Image Analysis for High-Throughput Plant Phenotyping"]},{id:9,source:"https://github.com/lameski/rgbweeddetection",name:"Weed detection dataset with RGB images taken under variable light conditions",description:'A dataset of RGB images from carrot seedlings with weeds (Carrot-Weed dataset) taken from farms near Negotino, Macedonia. The dataset was published in: Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik, Andrea Kulakov. 2017. "Weed detection dataset with RGB images taken under variable light conditions"',author:"Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik, Andrea Kulakov",data_type:"RGB images",num_of_images:"39 images",num_of_plants:"more than 100",num_of_species:"1 (carrot)",data_source:"taken by a cell phone camera with 10 mega pixels (from 1 meter height). Ground vehicle, under constant light condition with near infrared (NIR) and Red (R) channels",location:"Negotino, Republic of Macedonia (in February)",condition_of_use:"The dataset can be used freely for non-commercial applications",manually_labelled:"yes",citation:["Petre Lameski, Eftim Zdravevski,Vladimir Trajkovik, Andrea Kulakov. 2017. Weed Detection Dataset with RGB Images Taken Under Variable Light Conditions","Eftim Zdravevski. 2017. Parallelization of algorithms for data processing and classification","Petre Lameski. 2017. Plant Species Recognition Based on Machine Learning and Image Processing","Vladimir Trajkovik, Eftim Zdravevski, Petre Lameski, Andrea Kulakov. 2018. Cloud based Data Acquisition and Annotation Architecture for Weed Control","Nicol\xe1s Corti, Florian Brunner, Simon Baier, Juergen Geist. 2018. Quantification of Extent, Density, and Status of Aquatic Reed Beds Using Point Clouds Derived from UAV\u2013RGB Imagery","Jane Lameski, Andrej Jovanov, Eftim Zdravevski, Petre Lameski, S. Gievska. 2019. Skin lesion segmentation with deep learning"]},{id:10,source:"https://data.mendeley.com/datasets/3fmjm7ncc6/2",name:"Weed Detection in Soybean Crops",description:"From the set of images captured by the UAV, all those with occurrence of weeds were selected resulting a total of 400 images. Through the Pynovis\xe3o software, using the SLIC algorithm, these images were segmented and the segments annotated manually with their respective class. These segments were used in the construction of the image dataset.",author:"Alessandro dos Santos Ferreira, Hemerson Pistori, Daniel Matte Freitas and Gercina Gon\xe7alves da Silva",data_type:"RGB images",num_of_images:"400 images",num_of_plants:"n/a",num_of_species:"3 (soybean, grass, broadleaf)",data_source:"taken by unmanned aerial vehicles",location:"Brazil",condition_of_use:"free to use",manually_labelled:"yes",citation:["Alex Olsen, Dmitry Konovalov, Bronson Philippa, Ron D White. 2018. DeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning","Saad Abouzahir, Mohamed Sadik, Essaid Sabir. 2018. Enhanced Approach for Weeds Species Detection Using Machine Vision","Shaun M. SharpeArnold W. Schumann, Jialin Yu, Nathan S. Boyd. Vegetation detection and discrimination within vegetable plasticulture row-middles using a convolutional neural network","Mamadou Dian Bah, Adel Hafiane, R. Canals. 2018. Deep Learning with Unsupervised Data Labeling for Weed Detection in Line Crops in UAV Images","Corey LammieTony Carrick, Mostafa Rahimi Azghadi, Alex Olsen. 2019. Low-Power and High-Speed Deep FPGA Inference Engines for Weed Classification at the Edge","Alwaseela Abdalla, Haiyan Cen, Liang Wan, Leon He. 2019. Fine-tuning convolutional neural network with transfer learning for semantic segmentation of ground-level oilseed rape images in a field with high weed pressure"]},{id:11,source:"http://www.dis.uniroma1.it/~labrococo/fds/syntheticdatasets.html",name:"Synthetic Datasets",description:'A collection of real and synthetic weeds images. The dataset accompanies the following paper: "Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto. 2017. Automatic Model Based Dataset Generation for Fast and AccurateCrop and Weeds Detection."',author:"Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto.",data_type:"RGB images",num_of_images:"869M of images",num_of_plants:"n/a",num_of_species:"3 (sugarbeet, Galium Aparinae weeds, Capsella Bursa Pastoris weeds",data_source:"ground vehicle with camera attached",location:"Italia",condition_of_use:"free to use",manually_labelled:"yes",citation:["Nan Li, Xiaoguang Zhang, Chunlong Zhang, Xinyu Wu. 2019. Real-time crop recognition in transplanted fields with prominent weed growth: a visual-attention-based approach","Jose Pena, Yumin Tan, Wuttichai Boonpook. 2019. Semantic Segmentation Based Remote Sensing Data Fusion on Crops Detection","Lars Grimstad, Remy Zakaria, Tuan Dung Le, P\xe5l Johan From. 2018. A Novel Autonomous Robot for Greenhouse Applications","Aichen Wang, Wen Zhang, Xinhua Wei. 2019. A review on weed detection using ground-based machine vision and image processing techniques"]}],E=Object(r.a)({table:{minWidth:650},tablecell:{fontSize:"12pt"}});function k(){var e=E();return i.a.createElement("div",null,i.a.createElement(_,null),i.a.createElement("header",{className:"bg-primary py-5 mb-5"},i.a.createElement("div",{className:"container h-100"},i.a.createElement("div",{className:"row h-100 align-items-center"},i.a.createElement("div",{className:"col-lg-12"},i.a.createElement("h1",{className:"display-4 text-white mt-5 mb-2"},i.a.createElement("br",null),i.a.createElement("div",null,"This website summarize a list of in-field weeds detection datasets.")),i.a.createElement("br",null),i.a.createElement("p",{className:"lead mb-5 text-white-50",id:"website-description"},"It takes a lot of effort to find an appropriate dataset for a data science/machine learning project. To make it easier for data scientists whose interest lies in automatic weeds detection, this website was created as a collection of weeds datasets, each with its own clear description and source details."))))),i.a.createElement(m.a,{component:p.a},i.a.createElement(l.a,{className:e.table,"aria-label":"simple table"},i.a.createElement(u.a,{id:"headrow"},i.a.createElement(h.a,null,i.a.createElement(d.a,{className:e.tablecell},"ID"),i.a.createElement(d.a,{align:"left",className:e.tablecell},"Dataset"),i.a.createElement(d.a,{align:"left",className:e.tablecell},"Number of species"),i.a.createElement(d.a,{align:"left",className:e.tablecell},"Number of images"),i.a.createElement(d.a,{align:"left",className:e.tablecell},"Number of plants"))),i.a.createElement(c.a,null,A.map((function(a){return i.a.createElement(h.a,{key:a.id},i.a.createElement(d.a,{component:"th",scope:"row",className:e.tablecell},a.id),i.a.createElement(d.a,{align:"left",className:e.tablecell},i.a.createElement(g.b,{to:"/des/".concat(a.id)},a.name)),i.a.createElement(d.a,{align:"left",className:e.tablecell},a.num_of_species),i.a.createElement(d.a,{align:"left",className:e.tablecell},a.num_of_images),i.a.createElement(d.a,{align:"left",className:e.tablecell},a.num_of_plants))}))))))}var D=function(e){function a(){return Object(f.a)(this,a),Object(y.a)(this,Object(v.a)(a).apply(this,arguments))}return Object(w.a)(a,e),Object(b.a)(a,[{key:"render",value:function(){var e=A[this.props.match.params.value-1];return i.a.createElement("div",null,i.a.createElement(_,null),i.a.createElement("header",{className:"bg-primary py-5 mb-5"},i.a.createElement("div",{className:"container h-100"},i.a.createElement("div",{className:"row h-100 align-items-center"},i.a.createElement("div",{className:"col-lg-12"},i.a.createElement("h1",{className:"display-4 text-white mt-5 mb-2"},i.a.createElement("br",null),e.name),i.a.createElement("br",null),i.a.createElement("p",{className:"lead mb-5 text-white-50",id:"dataset-description"},e.description),i.a.createElement("p",{className:"lead mb-5 text-white-50"},"Source:"," ",i.a.createElement("a",{id:"sourceLink103",href:e.source},e.source)))))),i.a.createElement("div",{className:"container"},i.a.createElement("div",{className:"row"},i.a.createElement("div",{className:"col-md-8 mb-5"},i.a.createElement("h2",null,"Basic Information"),i.a.createElement("hr",null),i.a.createElement("div",{className:"info_box"},i.a.createElement("p",null,"Author: ",e.author),i.a.createElement("p",null,"Data type: ",e.data_type),i.a.createElement("p",null,"Number of images: ",e.num_of_images),i.a.createElement("p",null,"Number of plants: ",e.num_of_plants),i.a.createElement("p",null,"Number of species: ",e.num_of_species),i.a.createElement("p",null,"Source of data: ",e.data_source),i.a.createElement("p",null,"Location where data was taken: ",e.location),i.a.createElement("p",null,"Conditions of use: ",e.condition_of_use),i.a.createElement("p",null,"Manually labelled: ",e.manually_labelled))),i.a.createElement("div",{className:"col-md-4 mb-5"},i.a.createElement("h2",null,"Citation"),i.a.createElement("hr",null),i.a.createElement("div",{className:"info_box",id:"cite"},"All citations are as of January, 2020:",i.a.createElement("br",null),i.a.createElement("br",null),e.citation.map((function(e){return i.a.createElement("p",{key:e},"- ",e)})))))))}}]),a}(i.a.Component),S=function(e){function a(){return Object(f.a)(this,a),Object(y.a)(this,Object(v.a)(a).apply(this,arguments))}return Object(w.a)(a,e),Object(b.a)(a,[{key:"render",value:function(){return i.a.createElement("body",null,i.a.createElement(_,null),i.a.createElement("header",{className:"bg-primary py-5 mb-5"},i.a.createElement("div",{className:"container h-100"},i.a.createElement("div",{className:"row h-100 align-items-center"},i.a.createElement("div",{className:"col-lg-12"},i.a.createElement("h1",{className:"display-4 text-white mt-5 mb-2"},i.a.createElement("br",null),"Contact"),i.a.createElement("p",{className:"lead mb-5 text-white-50"},"Author: Van Manh Nguyen",i.a.createElement("br",null),"Email: mike.manhnguyen103@gmail.com"),i.a.createElement("br",null),i.a.createElement("p",{className:"lead mb-5 text-white-50",id:"contact-message"},"This website serves as a hub for everyone who is interested in automatic weed detection (or data science and machine learning in general). There are plenty of valuable datasets out there that I may have missed. If you know of any of those, or have any feedbacks, please do not hesitate to contact me, so that we can make this website better for every one who needs it. Thank you."))))))}}]),a}(i.a.Component),P=t(14);t(44);var N=document.getElementById("root");s.a.render(i.a.createElement((function(){return i.a.createElement("div",{className:"App"},i.a.createElement(g.a,{basename:"/weed-datasets"},i.a.createElement(P.a,{path:"/",exact:!0,component:k}),i.a.createElement(P.a,{path:"/des/:value",component:D}),i.a.createElement(P.a,{path:"/contact",component:S})))}),null),N)}},[[34,1,2]]]);
//# sourceMappingURL=main.b3d0a42a.chunk.js.map